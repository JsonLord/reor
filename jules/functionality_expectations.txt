### **Functionality Expectations of the Deployed Application**

| **Feature**               | **Description**                                                                                                                                                                                                                                                                 |
|---------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Core AI Features**      | - **RAG (Retrieval-Augmented Generation)**: Uses the `alias-large` model via BLABLADOR’s OpenAI-compatible API to answer queries based on retrieved notes.
|                           | - **Semantic Search**: Vector embeddings stored in LanceDB enable semantic search across all notes.
|                           | - **Automatic Linking**: Related notes are automatically connected via vector similarity.
|                           | - **Human + AI Co-generation**: "Editor mode" allows toggling a sidebar to reveal related notes (retrieved from the corpus). |
| **API Access**            | - **OpenAI-Compatible API Endpoints**:
|                           |   - `/api/embeddings`: Generate embeddings for text chunks.
|                           |   - `/api/query`: RAG-powered question-answering using BLABLADOR.
|                           |   - `/api/search`: Semantic search (LanceDB-backed).
|                           |   - `/api/health`: Check backend status.
| **Local Persistence**     | - All notes and embeddings stored locally in LanceDB (unchanged from Reor’s design).
| **AI-Powered Q&A**        | - LLM (BLABLADOR) retrieves relevant context from the corpus to answer user queries.
| **Editor Mode Integration**| - Sidebar displays "retrieved" related notes via semantic search (enhancing note-taking workflow).
| **BLABLADOR LLM Integration** | - Replaces Ollama/OpenAI support in Reor’s backend, using the `alias-large` model via:
|                             |   - **Endpoint**: `https://api.helmholtz-blablador.fz-juelich.de/v1`
|                             |   - **Authentication**: `BLABLADOR_API_KEY` (injected via Hugging Face Space environment variables).
| **Deployment Requirements**| - **Frontend**: Gradio UI (port `7860`) with Obsidian-like markdown editor.
|                           | - **Backend**: FastAPI/Flask layer exposing APIs under `/api/*` (port `8000`).
|                           | - **Reverse Proxy**: Nginx forwards:
|                             |   - `/` → Gradio (UI, port `7860`).
|                             |   - `/api/*` → FastAPI (backend, port `8000`).
| **Security & Compliance** | - **API Key Validation**: Fails if `BLABLADOR_API_KEY` is missing.
|                           | - **Rate Limiting**: Protects endpoints (e.g., 100 requests/hour).
|                           | - **Input Sanitization**: Prevents injection in markdown files.
| **Docker & Deployment**   | - Single container with Nginx (proxy), FastAPI (backend), and Gradio (UI).
|                           | - Deployed to **Hugging Face Space**: `harvesthealth-magnetic-ui.hf.space` (port `7860` exposed).
| **Monitoring**            | - Logs API usage (e.g., `user_id: query_text`) to Hugging Face logs.
|                           | - Alerts on failures (e.g., BLABLADOR timeouts) via webhooks.