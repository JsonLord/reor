### **1. Estimation of Project Scope (1-10) & Core Parts**
**Scope Rating**: **8/10** (High complexity due to integration of external API, RAG logic, and Hugging Face deployment constraints).
**Core Parts**:
- **Backend Replacement**: Replace Ollama calls with OpenAI-compatible API (`BLABLADOR`).
- **Vector Embeddings**: Retain LanceDB for local storage.
- **RAG Pipeline**: Implement semantic search, note chunking, and Q&A.
- **API Layer**: Expose FastAPI endpoints for external interaction.
- **Deployment**: Docker + Hugging Face Space with port forwarding.

---

### **2. Project Description**
**Vision**:
A **privacy-first**, **locally hosted** AI knowledge manager that integrates with `BLABLADOR`'s OpenAI-compatible API, enabling:
- **Semantic search** via vector embeddings (LanceDB).
- **RAG-based Q&A** (retrieve + generate) for notes.
- **Obsidian-like markdown editor** with related note retrieval.

**Concrete Goals**:
- Replace Ollama with `BLABLADOR_API_KEY` for LLM inference.
- Expose **FastAPI endpoints** (`/chat`, `/search`, `/notes`) for Hugging Face Space.
- Maintain **local-first** data storage (no cloud dependency).

**Future Use Cases**:
- **Enterprise Knowledge Bases**: Deploy on private Hugging Face Spaces for secure internal use.
- **Cross-App Integrations**: Plugins for Notion, Obsidian, or Jupyter via API.
- **Custom Models**: Extend to support other OpenAI-compatible APIs (e.g., Oobabooga).

---

### **3. External Projects/APIs to Integrate**
| **Component**          | **Purpose**                                                                 | **Integration Notes**                                                                                     |
|------------------------|-----------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------|
| **BLABLADOR API**      | LLM inference (OpenAI-compatible)                                         | Use `requests` to call `https://api.helmholtz-blablador.fz-juelich.de/v1` with `alias-large`.           |
| **LanceDB**            | Vector embeddings for semantic search                                      | Local installation (or cloud-hosted for Hugging Face).                                                |
| **FastAPI**            | Backend API layer                                                           | Define `/chat`, `/search`, `/notes` endpoints.                                                       |
| **Gradio/Streamlit**   | Frontend UI                                                                 | Gradio preferred for API-friendly deployment (port `7860`).                                              |
| **Hugging Face Space** | Hosting + reverse proxy                                                     | Deploy via Docker, expose `/chat` endpoint at `https://harvesthealth-magnetic-ui.hf.space/chat`.      |

---

### **4. Component Breakdown**
#### **4.1. Core Components to Build**
1. **Backend (FastAPI)**:
   - API routes for `/chat`, `/search`, `/notes`.
   - Integration with `BLABLADOR_API_KEY` and LanceDB.
2. **Vector DB (LanceDB)**:
   - Store and query embeddings for semantic search.
3. **Frontend (Gradio)**:
   - Markdown editor + sidebar for related notes (RAG).
4. **Docker Image**:
   - Containerize FastAPI + Gradio + LanceDB.
5. **Hugging Face Space**:
   - Configure `space.yml` for port forwarding (`7860` → `/chat`).

#### **4.2. Subtasks per Component**
**Backend (FastAPI)**:
- [ ] Replace Ollama calls with `BLABLADOR_API_KEY` (use `requests`).
- [ ] Implement **RAG pipeline**:
  - Chunk notes → embed with `BLABLADOR` → store in LanceDB.
  - Query LanceDB for semantic search.
- [ ] Secure API keys via environment variables.
- [ ] **Architecture**:
  - `POST /chat`: Accept prompt → retrieve context → generate response.
  - `GET /search`: Return top-k similar notes.
  - `POST /notes`: CRUD for markdown files.

**LanceDB**:
- [ ] Initialize local DB for embeddings.
- [ ] Test vector similarity queries.

**Frontend (Gradio)**:
- [ ] Build UI with:
  - Text input for prompts.
  - Markdown preview.
  - Sidebar for related notes (from `/search` endpoint).
- [ ] Connect to FastAPI via `/chat`.

**Docker**:
- [ ] `Dockerfile` with dependencies (FastAPI, LanceDB, Gradio).
- [ ] Multi-stage build to minimize size.

**Hugging Face Space**:
- [ ] Configure `space.yml` to forward `7860` → `/chat`.
- [ ] Set `BLABLADOR_API_KEY` in Hugging Face env vars.

#### **4.3. Tests per Component**
| **Component**       | **Test Description**                                                                 | **Success Criteria**                                                                 |
|---------------------|------------------------------------------------------------------------------------|------------------------------------------------------------------------------------|
| **Backend API**     | `POST /chat` with prompt → returns LLM response.                                   | Response contains generated text from `BLABLADOR`.                                 |
|                     | `GET /search` with query → returns top-3 similar notes.                           | Notes match semantic content (checked via `cosine_similarity`).                    |
|                     | `POST /notes` with markdown → saves file locally.                                 | File exists in storage directory.                                                 |
| **LanceDB**         | Insert 10 embeddings → query with semantic term → returns correct matches.        | Top match is within 0.1 cosine distance of expected vector.                         |
| **Frontend (Gradio)** | Submit prompt → UI displays LLM response + related notes.                        | Response appears in chat box; sidebar shows 2+ notes.                               |
| **Docker**          | Build image → run container → APIs accessible on port `7860`.                     | `curl localhost:7860/chat` returns JSON response.                                  |
| **Hugging Face**    | Deploy space → access `https://harvesthealth-magnetic-ui.hf.space/chat`.         | API endpoint returns expected data (no 500 errors).                                  |

---

### **5. Full Pipeline Test**
**Input**:
- User uploads 3 markdown notes (`note1.md`, `note2.md`, `note3.md`).
- Notes contain overlapping topics (e.g., "AI", "knowledge management").
- User asks: `"Explain the concept of RAG in my notes."`

**Expected Output**:
1. **Backend**:
   - Chunks notes → embeds with `BLABLADOR` → stores in LanceDB.
   - Retrieves top-2 relevant notes via semantic search.
   - Passes context + prompt to `BLABLADOR_API_KEY`:
     ```json
     {
       "model": "alias-large",
       "messages": [
         {"role": "system", "content": "You are an expert on knowledge management."},
         {"role": "user", "content": "Explain RAG using context: [note1_embed] [note2_embed]"}
       ]
     }
     ```
2. **Frontend**:
   - Gradio UI displays:
     - **Chat response**: Concise RAG-based explanation.
     - **Sidebar**: `note1.md` and `note2.md` (highlighted as sources).

**Mock Input Data** (for testing):
```markdown
# note1.md
## RAG in AI
Retrieval-Augmented Generation combines **search** and **LLMs**.
Steps:
1. Retrieve relevant documents.
2. Pass to LLM for response.
```

```markdown
# note2.md
## Knowledge Management
Tools like **Obsidian** and **Reor** use vectors for semantic links.
```

**Success Criteria**:
- API response contains **"retrieve"** and **"generate"** in answer.
- Sidebar shows `note1.md` and `note2.md` with confidence scores > 0.9.

---

### **6. Task: API Endpoints**
**FastAPI Endpoints** (to be tested via Hugging Face Space):
| **Endpoint**       | **Method** | **Description**                                                                 | **Request Example**                                                                 | **Expected Response**                                                                 |
|--------------------|------------|---------------------------------------------------------------------------------|------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------|
| `/chat`            | POST       | RAG-based Q&A.                                                                | `{"prompt": "Explain RAG"}`, `{"notes": ["note1.md"]}`                          | `{"response": "RAG combines retrieval..."}`                                         |
| `/search`          | GET        | Semantic search for notes.                                                    | `?query="AI"`                                                                       | `[{"id": "note1", "title": "RAG in AI", "score": 0.99}]`                            |
| `/notes`           | POST       | Create/update a note.                                                          | `{"content": "# Test", "filename": "test.md"}`                                    | `{"status": "success"}`                                                              |
| `/notes/{id}`      | GET        | Retrieve a note by ID.                                                         | `/notes/1`                                                                         | `{"content": "# Test", "filename": "test.md"}`                                      |
| `/embeddings`      | POST       | Add embeddings to LanceDB.                                                     | `{"text": "RAG is...", "note_id": "1"}`                                           | `{"status": "inserted"}`                                                             |

**H